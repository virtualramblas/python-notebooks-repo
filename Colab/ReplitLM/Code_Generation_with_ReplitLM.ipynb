{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Generation with ReplitLM\n",
        "A notebook to generate code in any of the programming languages for which the [ReplitLM](https://github.com/replit/ReplitLM) model has been trained.  \n",
        "No hardware acceleration required.  "
      ],
      "metadata": {
        "id": "-3mn1T4ZySgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VMs."
      ],
      "metadata": {
        "id": "xx79u_4lj_IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece einops accelerate"
      ],
      "metadata": {
        "id": "9U6ItlZ7kDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model and tokenizer using the Hugging Face's Transformer library."
      ],
      "metadata": {
        "id": "4mi9WYGSj50n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9MOpgmjjmR2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "\n",
        "model_id = \"replit/replit-code-v1-3b\"\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "UNK_TOKEN = \"<|unk|>\"\n",
        "MAX_INPUT_TOKENS = 1024\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.truncation_side = \"left\" # ensures if truncate, then keep the last N tokens of the prompt going L -> R\n",
        "if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, low_cpu_mem_usage=True).to(device, dtype=torch.bfloat16)\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, low_cpu_mem_usage=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the prompt and the values of other model parameters."
      ],
      "metadata": {
        "id": "vDOmfexBk1nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'def sieve_eratosthenes(n):' #@param {type:\"string\"}\n",
        "max_tokens = 112 #@param {type:\"slider\", min:10, max:1024, step:1}\n",
        "temperature = 0.2 #@param {type:\"slider\", min:0.1, max:2.5, step:0.1}\n",
        "seed = 42 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "top_k = 4 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "repetition_penalty = 1.0 #@param {type:\"slider\", min:1.0, max:1.9, step:0.1}\n",
        "use_cache = True #@param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "HA4ztsPhk8Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate code."
      ],
      "metadata": {
        "id": "cffzlKHVlFno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=MAX_INPUT_TOKENS, truncation=True).to(device)\n",
        "set_seed(seed)\n",
        "model_output = model.generate(model_input, \n",
        "                   max_length=max_tokens, \n",
        "                   do_sample=True, \n",
        "                   top_p=top_p, \n",
        "                   top_k=top_k, \n",
        "                   temperature=temperature, \n",
        "                   num_return_sequences=1, \n",
        "                   repetition_penalty = repetition_penalty,\n",
        "                   pad_token_id=tokenizer.pad_token_id,\n",
        "                   eos_token_id=tokenizer.eos_token_id,\n",
        "                   use_cache=use_cache,)"
      ],
      "metadata": {
        "id": "O4qwOPHvlH5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode the generated output."
      ],
      "metadata": {
        "id": "Eeede5DdlPyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_code = tokenizer.decode(model_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_code)\n",
        "prompt = generated_code"
      ],
      "metadata": {
        "id": "7woibwtKlTDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}