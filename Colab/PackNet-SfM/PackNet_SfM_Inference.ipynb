{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PackNet-SfM - Inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDWZpb_qEY3Y"
      },
      "source": [
        "# Inference with PackNet-SfM\n",
        "A notebook for doing inference on your own images using the Toyota's **[PackNet-SfM](https://arxiv.org/abs/1905.02693)** official PyTorch implementation, a 3D packing for self-supervised monocular Depth Estimation.  \n",
        "\n",
        "\n",
        "```\n",
        "@inproceedings{packnet,\n",
        "  author = {Vitor Guizilini and Rares Ambrus and Sudeep Pillai and Allan Raventos and Adrien Gaidon},\n",
        "  title = {3D Packing for Self-Supervised Monocular Depth Estimation},\n",
        "  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
        "  primaryClass = {cs.CV}\n",
        "  year = {2020},\n",
        "}\n",
        "```  \n",
        "The official GitHub repository for this work includes instructions on how to set a reproducible environment for training/evaluation/inference using Docker. The purpose of this notebook is to provide an alternative setup to Docker within Colab.  \n",
        "A GPU runtime is required to run the code in this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AirktvqdOOF"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcvuxcIbEoIl"
      },
      "source": [
        "Clone the official GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yh7rSRcD6Rb"
      },
      "source": [
        "!git clone https://github.com/TRI-ML/packnet-sfm.git\n",
        "%cd packnet-sfm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKmxwv_vtK8-"
      },
      "source": [
        "Install the missing dependencies. All the mandatory requirements for PackNet-SfM are already available in the Colab virtual envinronment, but *yacs*. We need also to install *python-wget* for checkpoints donwload. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRkmEU5ptNMj"
      },
      "source": [
        "!pip install yacs\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htYqLUsddfYb"
      },
      "source": [
        "Add the *packet_sfm* package to the Python sys.path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhV94yjIq_TM"
      },
      "source": [
        "%set_env PYTHONPATH=/content/packnet-sfm/packnet_sfm:/env/python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1h-iUApEtKL"
      },
      "source": [
        "Create the model's checkpoint directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2e7-XzUE9yl"
      },
      "source": [
        "!mkdir ./checkpoints\n",
        "checkpoints_dir = './checkpoints'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SbkOaExG5x3"
      },
      "source": [
        "Create the input and output directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbShf_gG-Ih"
      },
      "source": [
        "!mkdir ./input\n",
        "!mkdir ./output\n",
        "image_input_dir = './input'\n",
        "image_output_dir = './output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxEkfc4YgGoN"
      },
      "source": [
        "## Source Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTG9xri3gMIf"
      },
      "source": [
        "Define a function to upload your own images in the input directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeuIBg3tGUX7"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "def upload_files(upload_path):\n",
        "  uploaded = files.upload()\n",
        "  for filename, content in uploaded.items():\n",
        "    dst_path = os.path.join(upload_path, filename)\n",
        "    shutil.move(filename, dst_path)\n",
        "  return list(uploaded.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF-jksf3gV0i"
      },
      "source": [
        "Define a function to check if a given file is an image having a certain extension and another one to do inference and optionally save the depth map only to file. These 2 functions have been copied from the */content/packnet-sfm/scripts/infer.py* file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4-F3No5tnhc"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from glob import glob\n",
        "from cv2 import imwrite\n",
        "\n",
        "from packnet_sfm.models.model_wrapper import ModelWrapper\n",
        "from packnet_sfm.datasets.augmentations import resize_image, to_tensor\n",
        "from packnet_sfm.utils.horovod import hvd_init, rank, world_size, print0\n",
        "from packnet_sfm.utils.image import load_image\n",
        "from packnet_sfm.utils.config import parse_test_file\n",
        "from packnet_sfm.utils.load import set_debug\n",
        "from packnet_sfm.utils.depth import write_depth, inv2depth, viz_inv_depth\n",
        "from packnet_sfm.utils.logging import pcolor\n",
        "\n",
        "\n",
        "def is_image(file, ext=('.png', '.jpg',)):\n",
        "    \"\"\"Check if a file is an image with certain extensions\"\"\"\n",
        "    return file.endswith(ext)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_and_save_depth(input_file, output_file, model_wrapper, image_shape, half, save):\n",
        "    \"\"\"\n",
        "    Process a single input file to produce and save visualization\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_file : str\n",
        "        Image file\n",
        "    output_file : str\n",
        "        Output file, or folder where the output will be saved\n",
        "    model_wrapper : nn.Module\n",
        "        Model wrapper used for inference\n",
        "    image_shape : Image shape\n",
        "        Input image shape\n",
        "    half: bool\n",
        "        use half precision (fp16)\n",
        "    save: str\n",
        "        Save format (npz or png)\n",
        "    \"\"\"\n",
        "    if not is_image(output_file):\n",
        "        # If not an image, assume it's a folder and append the input name\n",
        "        os.makedirs(output_file, exist_ok=True)\n",
        "        output_file = os.path.join(output_file, os.path.basename(input_file))\n",
        "\n",
        "    # change to half precision for evaluation if requested\n",
        "    dtype = torch.float16 if half else None\n",
        "\n",
        "    # Load image\n",
        "    image = load_image(input_file)\n",
        "    # Resize and to tensor\n",
        "    image = resize_image(image, image_shape)\n",
        "    image = to_tensor(image).unsqueeze(0)\n",
        "\n",
        "    # Send image to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        image = image.to('cuda:{}'.format(rank()), dtype=dtype)\n",
        "\n",
        "    # Depth inference (returns predicted inverse depth)\n",
        "    pred_inv_depth = model_wrapper.depth(image)['inv_depths'][0]\n",
        "\n",
        "    if save == 'npz' or save == 'png':\n",
        "        # Get depth from predicted depth map and save to different formats\n",
        "        filename = '{}.{}'.format(os.path.splitext(output_file)[0], save)\n",
        "        print('Saving {} to {}'.format(\n",
        "            pcolor(input_file, 'cyan', attrs=['bold']),\n",
        "            pcolor(filename, 'magenta', attrs=['bold'])))\n",
        "        write_depth(filename, depth=inv2depth(pred_inv_depth))\n",
        "    else:\n",
        "        # Prepare RGB image\n",
        "        rgb = image[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "        # Prepare inverse depth\n",
        "        viz_pred_inv_depth = viz_inv_depth(pred_inv_depth[0]) * 255\n",
        "        # Concatenate both vertically\n",
        "        image = np.concatenate([rgb, viz_pred_inv_depth], 0)\n",
        "        # Save visualization\n",
        "        print('Saving {} to {}'.format(\n",
        "            pcolor(input_file, 'cyan', attrs=['bold']),\n",
        "            pcolor(output_file, 'magenta', attrs=['bold'])))\n",
        "        imwrite(output_file, image[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpLoAt5Bi8hE"
      },
      "source": [
        "Define a function for model initialization and inference settings. This is a modified version of the same available in the original script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiB-H_X-i89H"
      },
      "source": [
        "def do_inference(checkpoint, input, output, image_shape=None, half=None, save=None):\n",
        "\n",
        "    # Initialize horovod\n",
        "    hvd_init()\n",
        "\n",
        "    # Parse the checkpoint file\n",
        "    config, state_dict = parse_test_file(checkpoint)\n",
        "\n",
        "    # If no image shape is provided, use the checkpoint one\n",
        "    if image_shape is None:\n",
        "        image_shape = config.datasets.augmentation.image_shape\n",
        "\n",
        "    # Set debug if requested\n",
        "    set_debug(config.debug)\n",
        "\n",
        "    # Initialize model wrapper from checkpoint arguments\n",
        "    model_wrapper = ModelWrapper(config, load_datasets=False)\n",
        "    # Restore monodepth_model state\n",
        "    model_wrapper.load_state_dict(state_dict)\n",
        "\n",
        "    # change to half precision for evaluation if requested\n",
        "    if half == \"No\":\n",
        "        half = False\n",
        "    dtype = torch.float16 if half else None\n",
        "\n",
        "    # Send model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model_wrapper = model_wrapper.to('cuda:{}'.format(rank()), dtype=dtype)\n",
        "\n",
        "    # Set to eval mode\n",
        "    model_wrapper.eval()\n",
        "\n",
        "    if os.path.isdir(input):\n",
        "        # If input file is a folder, search for image files\n",
        "        files = []\n",
        "        for ext in ['png', 'jpg']:\n",
        "            files.extend(glob((os.path.join(input, '*.{}'.format(ext)))))\n",
        "        files.sort()\n",
        "        print0('Found {} files'.format(len(files)))\n",
        "    else:\n",
        "        # Otherwise, use it as is\n",
        "        files = [input]\n",
        "\n",
        "    # Process each file\n",
        "    for fn in files[rank()::world_size()]:\n",
        "        infer_and_save_depth(\n",
        "            fn, output, model_wrapper, image_shape, half, save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srJDks5kNx0"
      },
      "source": [
        "Create a checkpoint dictionary, as multiple pre-trained models are available for download. This dictionary will be used by the settings form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkE4OUFQ_bOX"
      },
      "source": [
        "checkpoint_dict = {\n",
        "    \"ResNet18 Self-Supervised 384x640 ImageNet DDAD\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/ResNet18_MR_selfsup_D.ckpt\", \n",
        "    \"PackNet Self-Supervised 384x640 DDAD\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_selfsup_D.ckpt\", \n",
        "    \"PackNetSAN Supervised 384x640 DDAD\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNetSAN01_HR_sup_D.ckpt\",\n",
        "    \"ResNet18 Self-Supervised 192x640 ImageNet KITTI\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/ResNet18_MR_selfsup_K.ckpt\",\n",
        "    \"PackNet Self-Supervised 192x640 KITTI\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_selfsup_K.ckpt\",\n",
        "    \"PackNet Self-Supervised Scale-Aware 192x640 CS K\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_velsup_CStoK.ckpt\",\n",
        "    \"PackNet Self-Supervised Scale-Aware 384x1280 CS K\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_HR_velsup_CStoK.ckpt\",\n",
        "    \"PackNet Semi-Supervised densified GT 192x640 CS K\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_semisup_CStoK.ckpt\",\n",
        "    \"PackNetSAN Supervised densified GT 352x1216 K\": \"https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNetSAN01_HR_sup_K.ckpt\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUYcR6zGU1V"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLa0gCcGJOB"
      },
      "source": [
        "Upload your own image(s). The will be saved in the */content/packnet-sfm/input/* directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO2zx73WJjxi"
      },
      "source": [
        "uploaded_image_list = upload_files(image_input_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvrHxGY0l-Qb"
      },
      "source": [
        "Build the inference settings form. Here it is possible to select model checkpoint, halv the precision and save a predicted depth map in a separate image file or NumPy compressed array format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4057uePP_hvF"
      },
      "source": [
        "#@title Inference Options\n",
        "\n",
        "checkpoint = 'ResNet18 Self-Supervised 384x640 ImageNet DDAD' #@param [\"ResNet18 Self-Supervised 384x640 ImageNet DDAD\", \"PackNet Self-Supervised 384x640 DDAD\", \"PackNetSAN Supervised 384x640 DDAD\", \"ResNet18 Self-Supervised 192x640 ImageNet KITTI\", \"PackNet Self-Supervised 192x640 KITTI\", \"PackNet Self-Supervised Scale-Aware 192x640 CS K\", \"PackNet Self-Supervised Scale-Aware 384x1280 CS K\", \"PackNet Semi-Supervised densified GT 192x640 CS K\", \"PackNetSAN Supervised densified GT 352x1216 K\"]\n",
        "half = \"No\" #@param [\"No\", \"Yes\"]\n",
        "save = \"None\" #@param [\"None\", \"png\", \"npz\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAj4_y5stsGU"
      },
      "source": [
        "Do inference on the uploaded images. Results are saved in the */content/packnet-sfm/output/* directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqMZfXYefOje"
      },
      "source": [
        "import wget\n",
        "\n",
        "selected_checkpoint_url = checkpoint_dict[checkpoint]\n",
        "last_file_sep_index = selected_checkpoint_url.rindex('/')\n",
        "checkpoint_filename = selected_checkpoint_url[last_file_sep_index + 1: ]\n",
        "checkpoint_full_path = checkpoints_dir + '/' + checkpoint_filename\n",
        "if not os.path.exists(checkpoint_full_path):\n",
        "  wget.download(selected_checkpoint_url, out=checkpoints_dir)\n",
        "do_inference(checkpoint_full_path, image_input_dir, image_output_dir, half=half, save=save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fyc_WcG7Sns"
      },
      "source": [
        "Display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1DMNuRv7VNF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "input_images_path = image_output_dir\n",
        "items = os.listdir(input_images_path)    \n",
        "\n",
        "for each_image in items:\n",
        "  if each_image.endswith(\".jpg\") or each_image.endswith(\".png\"):\n",
        "    print(each_image)\n",
        "    full_path = input_images_path + '/' + each_image\n",
        "    image = cv2.imread(full_path)\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.imshow(image)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}