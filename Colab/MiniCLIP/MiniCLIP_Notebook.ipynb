{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniCLIP Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LnzFjFIIHB-"
      },
      "source": [
        "## MiniCLIP to Colab\r\n",
        "A Colab notebook porting of the MiniCLIP demo (https://github.com/HendrikStrobelt/miniClip) from Hendrik Strobelt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpme4g6XIbQG"
      },
      "source": [
        "Import the missing dependencies.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRtdXt99H_k9"
      },
      "source": [
        "!pip install torchray "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqxxk4tXJYrZ"
      },
      "source": [
        "Install CLIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuvoziLfJbPc"
      },
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-xkdCpZKeJF"
      },
      "source": [
        "Add the MiniCLIP user defined functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GmuXS4hKid9"
      },
      "source": [
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from matplotlib import cm\r\n",
        "\r\n",
        "def min_max_norm(array):\r\n",
        "    lim = [array.min(), array.max()]\r\n",
        "    array = array - lim[0] \r\n",
        "    array.mul_(1 / (1.e-10+ (lim[1] - lim[0])))\r\n",
        "    # array = torch.clamp(array, min=0, max=1)\r\n",
        "    return array\r\n",
        "\r\n",
        "def torch_to_rgba(img):\r\n",
        "    img = min_max_norm(img)\r\n",
        "    rgba_im = img.permute(1, 2, 0).cpu()\r\n",
        "    if rgba_im.shape[2] == 3:\r\n",
        "        rgba_im = torch.cat((rgba_im, torch.ones(*rgba_im.shape[:2], 1)), dim=2)\r\n",
        "    assert rgba_im.shape[2] == 4\r\n",
        "    return rgba_im\r\n",
        "\r\n",
        "def numpy_to_image(img, size):\r\n",
        "    \"\"\"\r\n",
        "    takes a [0..1] normalized rgba input and returns resized image as [0...255] rgba image\r\n",
        "    \"\"\"\r\n",
        "    resized = Image.fromarray((img*255.).astype(np.uint8)).resize((size, size))\r\n",
        "    return resized\r\n",
        "\r\n",
        "def upscale_pytorch(img:np.array, size):\r\n",
        "    torch_img = torch.from_numpy(img).unsqueeze(0).permute(0,3,1,2)\r\n",
        "    print(torch_img)\r\n",
        "    upsampler = torch.nn.Upsample(size=size)    \r\n",
        "    return upsampler(torch_img)[0].permute(1,2,0).cpu().numpy()\r\n",
        "\r\n",
        "\r\n",
        "def heatmap(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\r\n",
        "    if not size:\r\n",
        "        size = image.shape[1]\r\n",
        "    # print(heatmap)\r\n",
        "    # print(min_max_norm(heatmap))\r\n",
        "\r\n",
        "    img = torch_to_rgba(image).numpy() # [0...1] rgba numpy \"image\"\r\n",
        "    hm = cm.hot(min_max_norm(heatmap).numpy()) # [0...1] rgba numpy \"image\"\r\n",
        "\r\n",
        "    # print(hm.shape, hm)\r\n",
        " #\r\n",
        "\r\n",
        "    img = np.array(numpy_to_image(img,size))\r\n",
        "    hm = np.array(numpy_to_image(hm, size))\r\n",
        "    # hm = upscale_pytorch(hm, size)\r\n",
        "    # print (hm) \r\n",
        "\r\n",
        "    #return Image.fromarray((alpha * hm + (1-alpha)*img).astype(np.uint8))\r\n",
        "    return Image.fromarray(hm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou2jNi3uKKUQ"
      },
      "source": [
        "Import other necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVaWm5fKKMwg"
      },
      "source": [
        "import clip\r\n",
        "from torchray.attribution.grad_cam import grad_cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88EACKVtLH_u"
      },
      "source": [
        "User defined function to get the CLIP ResNet50 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thqra1WYLNW2"
      },
      "source": [
        "def get_model():\r\n",
        "    return clip.load(\"RN50\", device=device, jit=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLUJNcH9N22I"
      },
      "source": [
        "User defined function to upload an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2TXJnWHOAGn"
      },
      "source": [
        "def upload_files():\r\n",
        "  from google.colab import files\r\n",
        "  uploaded = files.upload()\r\n",
        "  for k, v in uploaded.items():\r\n",
        "    open(k, 'wb').write(v)\r\n",
        "  return list(uploaded.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ui3gxPDLPK5"
      },
      "source": [
        "Set the device to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHwcOHwLR_Q"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQaxvnefLXQj"
      },
      "source": [
        "Build the Form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P_lWct7LYCP"
      },
      "source": [
        "#@title Options\r\n",
        "\r\n",
        "alpha = '0.5' #@param [\"0.5\", \"0,7\", \"0.8\"]\r\n",
        "layer = 'layer4.2.relu' #@param [\"layer4.2.relu\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjnEQ-y_OHLh"
      },
      "source": [
        "Upload an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4nOmE7lOJN2"
      },
      "source": [
        "uploaded_image_list = upload_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzpiM6nkPLwm"
      },
      "source": [
        "Enter some descriptive text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktYYW6KPP4q"
      },
      "source": [
        "#@title Enter some descriptive texts\r\n",
        "\r\n",
        "textarea = 'a pizza; a beer' #@param {type:\"string\"}\r\n",
        "prefix = 'an image of' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jmJkNPKOchM"
      },
      "source": [
        "Read and preprocess the uploaded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5byv-IjbOnx0"
      },
      "source": [
        "image_raw = Image.open(uploaded_image_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM4SPqA3Ovw-"
      },
      "source": [
        "model, preprocess = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko55OULhO6W-"
      },
      "source": [
        "image = preprocess(image_raw).unsqueeze(0).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcHqXCSvO9OS"
      },
      "source": [
        "Preprocess text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCS-Hjx0PnZN"
      },
      "source": [
        "prefix = prefix.strip()\r\n",
        "if len(prefix) > 0:\r\n",
        "        categories = [f\"{prefix} {x.strip()}\" for x in textarea.split(';')]\r\n",
        "else:\r\n",
        "        categories = [x.strip() for x in textarea.split(';')]\r\n",
        "text = clip.tokenize(categories).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ajGOLKQeb4"
      },
      "source": [
        "Calculate the saliency map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_aJ8yvkQe52"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "        image_features = model.encode_image(image)\r\n",
        "        text_features = model.encode_text(text)\r\n",
        "        image_features_norm = image_features.norm(dim=-1, keepdim=True)\r\n",
        "        image_features_new = image_features / image_features_norm\r\n",
        "        text_features_norm = text_features.norm(dim=-1, keepdim=True)\r\n",
        "        text_features_new = text_features / text_features_norm\r\n",
        "        logit_scale = model.logit_scale.exp()\r\n",
        "        logits_per_image = logit_scale * image_features_new @ text_features_new.t()\r\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy().tolist()\r\n",
        "\r\n",
        "saliency = grad_cam(model.visual, image.type(model.dtype), image_features, saliency_layer=layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ErqI-wbQ50Q"
      },
      "source": [
        "hm = heatmap(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsP1Mf5R_j6"
      },
      "source": [
        "Collect the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTr5uD1dR-dl"
      },
      "source": [
        "collect_images = []\r\n",
        "for i in range(len(categories)):\r\n",
        "    # mutliply the normalized text embedding with image norm to get approx image embedding\r\n",
        "    text_prediction = (text_features_new[[i]] * image_features_norm)\r\n",
        "    saliency = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\r\n",
        "    hm = heatmap(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)\r\n",
        "    collect_images.append(hm)\r\n",
        "logits = logits_per_image.cpu().numpy().tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iLjHdGITFd6"
      },
      "source": [
        "Show the Grad Cam for text embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9JDAEkSSkT"
      },
      "source": [
        "text_embeddings = [f\"{x} - {str(round(y, 3))}/{str(round(l, 2))}\" for (x, y, l) in\r\n",
        "                      zip(categories, probs[0], logits)]\r\n",
        "for image_idx in (0, len(collect_images) - 1):\r\n",
        "  display(collect_images[image_idx])\r\n",
        "  print(text_embeddings[image_idx])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9k5klM9Znl9"
      },
      "source": [
        "Show the original image and Grad Cam for image embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxx7DdusaeXg"
      },
      "source": [
        "display(Image.fromarray((torch_to_rgba(image[0]).numpy() * 255.).astype(np.uint8)), hm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}