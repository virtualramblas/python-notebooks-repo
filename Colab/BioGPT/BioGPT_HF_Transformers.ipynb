{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BioGPT with HF Transformers\n",
        "A notebook to evaluate [BioGPT](https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9), Microsoft's domain-specific generative Transformer language model pre-trained on large-scale biomedical literature.  \n",
        "No hardware acceleration needed to execute the code in this notebook."
      ],
      "metadata": {
        "id": "RrqxgftIonuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "id": "pkXi_eRNf4AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VM (Hugging Face's Transformer and sacremoses)."
      ],
      "metadata": {
        "id": "q1fjMgBVk3zk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jRokZ8Okq99"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary packages/classes."
      ],
      "metadata": {
        "id": "o8E0DnnSk8gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import BioGptTokenizer, BioGptForCausalLM"
      ],
      "metadata": {
        "id": "lx0NSUeWlDoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a pretrained model."
      ],
      "metadata": {
        "id": "ynv_4OCilEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
        "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")"
      ],
      "metadata": {
        "id": "xqyEyCLZlIRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation"
      ],
      "metadata": {
        "id": "urarK2o8FY-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the generation pipeline."
      ],
      "metadata": {
        "id": "SkjhNVn4lIxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "OFFsh7VGlQDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the prompt for the model, the maximum lenght of each generated text sequence and the maximum number of sequences to generate."
      ],
      "metadata": {
        "id": "2-V2RmTXgNSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Psoralen is\" #@param {type: \"string\"}\n",
        "generated_sequence_max_length = 60 #@param {type:\"slider\", min:10, max:200, step:1}\n",
        "num_return_sequences = 3 #@param {type:\"slider\", min:1, max:20, step:1}"
      ],
      "metadata": {
        "id": "sD0UbcHoBmSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text. The generated sequences are printed to the code cell output."
      ],
      "metadata": {
        "id": "DzxLj-l5lQ2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(prompt, \n",
        "          max_length=generated_sequence_max_length, \n",
        "          num_return_sequences=num_return_sequences, \n",
        "          do_sample=True)"
      ],
      "metadata": {
        "id": "TzNpoYvBlUrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam-Search Decoding"
      ],
      "metadata": {
        "id": "2MWYQvHMBI9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the minimum and max lenght of the generated text and the number of beams. The prompt for the model is the same set as for previous form for text generation."
      ],
      "metadata": {
        "id": "7bt3atDxgpQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_min_length = 100 #@param {type:\"slider\", min:10, max:200, step:1}\n",
        "generated_text_max_length = 1024 #@param {type:\"slider\", min:300, max:1200, step:1}\n",
        "num_beams = 5 #@param {type:\"slider\", min:1, max:10, step:1}"
      ],
      "metadata": {
        "id": "TgZjfnFhDwti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the feature of the given prompt in PyTorch format."
      ],
      "metadata": {
        "id": "IbP7ngGDhJKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "8x2sfCELBPP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do beam-search decoding. The generated text is printed to the code cell output."
      ],
      "metadata": {
        "id": "keoSgGc7hk1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    beam_output = model.generate(**inputs,\n",
        "                                min_length=generated_text_min_length,\n",
        "                                max_length=generated_text_max_length,\n",
        "                                num_beams=num_beams,\n",
        "                                early_stopping=True\n",
        "                                )\n",
        "tokenizer.decode(beam_output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "nRfQnx3PDlf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}