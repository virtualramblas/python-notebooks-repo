{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Object detection with Vision Transformer for Open-World Localization (OWL-ViT)\n",
        "The **[OWL-ViT](https://arxiv.org/abs/2205.06230)** model is an **open-vocabulary object detection model** that uses the standard Vision Transformer to perform detection.  \n",
        "![OWL-ViT](https://github.com/google-research/scenic/raw/main/scenic/projects/owl_vit/data/owl_vit_schematic.png)  \n",
        "Given an image and a free-text query, it finds objects matching that query in the image. It can also do one-shot object detection, i.e. detect objects based on a single example image. This notebook is to evaluate both tasks through the pre-trained model available in the Hugging Face's *transformers* library.   \n",
        "As a result of my evaluation of this model, it looks like it is robust with reference to the open-vocabulary object detection task, but not yet robust enough to perform accurate one-shot object detection.   \n",
        "A free Colab VM without hardware acceleration is enough to execute the code in this notebook."
      ],
      "metadata": {
        "id": "88X-tLr8gM8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "pJfqB0HLRo7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements (actually only *transformers* not present in the Colab VM)."
      ],
      "metadata": {
        "id": "qsXIa1hhRs_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYwEzTmdz2IZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the general packages/classes to use."
      ],
      "metadata": {
        "id": "EX9kQ9D2SKBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
      ],
      "metadata": {
        "id": "LTIQIjYD0AP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained ViT processor and model."
      ],
      "metadata": {
        "id": "XukaunmjSj3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")"
      ],
      "metadata": {
        "id": "wv-DwebY0Fel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to upload images."
      ],
      "metadata": {
        "id": "jSgwJjz70L3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def upload_files():\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(k, 'wb').write(v)\n",
        "  return list(uploaded.keys())"
      ],
      "metadata": {
        "id": "CFEjxYkJ0Na0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open-vocabulary Object Detection"
      ],
      "metadata": {
        "id": "EJva2nfHTI54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to add bounding boxes to the input image."
      ],
      "metadata": {
        "id": "Jqi466-oUDKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def add_bounding_boxes(boxes, scores, labels, text_queries, img):\n",
        "  img = np.array(img)\n",
        "\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "  for box, score, label in zip(boxes, scores, labels):\n",
        "      box = [int(i) for i in box.tolist()]\n",
        "\n",
        "      if score >= score_threshold:\n",
        "          img = cv2.rectangle(img, box[:2], box[2:], (255,0,0), 5)\n",
        "          if box[3] + 25 > 768:\n",
        "              y = box[3] - 10\n",
        "          else:\n",
        "              y = box[3] + 25\n",
        "              \n",
        "          img = cv2.putText(\n",
        "              img, text_queries[label], (box[0], y), font, 1, (255,0,0), 2, cv2.LINE_AA\n",
        "          )\n",
        "\n",
        "  return Image.fromarray(img, 'RGB')"
      ],
      "metadata": {
        "id": "4xqM5s5t2N-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload an image and display it."
      ],
      "metadata": {
        "id": "FM-tsjZGTOzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_image_list = upload_files()\n",
        "image = Image.open(uploaded_image_list[0])\n",
        "display(image)"
      ],
      "metadata": {
        "id": "OCbe4i4V0rfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicate a list of free texts for querying and the minimal score threshold for the object detection."
      ],
      "metadata": {
        "id": "BzEZjYnCTS5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [[\"tuxedo cat\", \"tabby cat\"]]\n",
        "score_threshold = 0.08"
      ],
      "metadata": {
        "id": "b0krqHxm0zeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do object detection on the uploaded image, querying by the list of choosen free texts."
      ],
      "metadata": {
        "id": "Lh0MJtshU72D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "V4ovVPUu020a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target image height and width to rescale box predictions and then convert the outputs (bounding boxes and class logits) to the COCO API format."
      ],
      "metadata": {
        "id": "LPhQzXKCVKH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_sizes = torch.Tensor([image.size[::-1]])\n",
        "\n",
        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)"
      ],
      "metadata": {
        "id": "kPGozPnu05Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the predictions for the input image by corresponding text queries."
      ],
      "metadata": {
        "id": "CIe_Twr8VwxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "text = texts[i]\n",
        "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
        "\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    if score >= score_threshold:\n",
        "        print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
      ],
      "metadata": {
        "id": "fkeXJSOU0-B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the bounding boxes for each prediction, overlay them to the input image and finally display it to the next code cell output."
      ],
      "metadata": {
        "id": "uMffKUJnWTzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelled_image = add_bounding_boxes(boxes, scores, labels, text, image)\n",
        "display(labelled_image)"
      ],
      "metadata": {
        "id": "KaJxHJ4y2Oxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image-guided Object Detection"
      ],
      "metadata": {
        "id": "mbTTa_r17bfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the target and query images and display them."
      ],
      "metadata": {
        "id": "n00M9rOqWp2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_image_guided_list = upload_files()\n",
        "\n",
        "target_image = Image.open(uploaded_image_guided_list[0])\n",
        "target_sizes = torch.Tensor([target_image.size[::-1]])\n",
        "\n",
        "query_image = Image.open(uploaded_image_guided_list[1])\n",
        "\n",
        "display(target_image)\n",
        "display(query_image)"
      ],
      "metadata": {
        "id": "ZHvsRkeD46Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the target and query images."
      ],
      "metadata": {
        "id": "BPre-sQXW_Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "inputs = processor(images=target_image, query_images=query_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")"
      ],
      "metadata": {
        "id": "OL14nbPj-lkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the predictions."
      ],
      "metadata": {
        "id": "FMAvA6F9XHmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model.image_guided_detection(**inputs)\n",
        "\n",
        "for k, val in outputs.items():\n",
        "    if k not in {\"text_model_output\", \"vision_model_output\"}:\n",
        "        print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nVision model outputs\")\n",
        "for k, val in outputs.vision_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\") "
      ],
      "metadata": {
        "id": "U15CFFT--pJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the bounding boxes for each prediction, overlay them to the target image and finally display it to the next code cell output."
      ],
      "metadata": {
        "id": "e0dyBtkNXRxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.cvtColor(np.array(target_image), cv2.COLOR_BGR2RGB)\n",
        "outputs.logits = outputs.logits.cpu()\n",
        "outputs.target_pred_boxes = outputs.target_pred_boxes.cpu() \n",
        "\n",
        "results = processor.post_process_image_guided_detection(outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes)\n",
        "boxes, scores = results[0][\"boxes\"], results[0][\"scores\"]\n",
        "\n",
        "for box, score in zip(boxes, scores):\n",
        "    box = [int(i) for i in box.tolist()]\n",
        "\n",
        "    img = cv2.rectangle(img, box[:2], box[2:], (255,0,0), 5)\n",
        "    if box[3] + 25 > 768:\n",
        "        y = box[3] - 10\n",
        "    else:\n",
        "        y = box[3] + 25 \n",
        "        \n",
        "display(Image.fromarray(img[:,:,::-1]))"
      ],
      "metadata": {
        "id": "rmsPUwje_PDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}