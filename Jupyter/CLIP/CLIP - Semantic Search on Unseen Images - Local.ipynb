{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from io import BytesIO\n",
    "\n",
    "import ftfy\n",
    "import ipyplot\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import requests\n",
    "import PIL\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CLIP model previously downloaded from [OpenAI](https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt). Replace the placeholder with the download location of the pre-trained model on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"<put the full path of the pre-trained CLIP model here>\").cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = model.input_resolution.item()\n",
    "context_length = model.context_length.item()\n",
    "vocab_size = model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the pixel intensity using the data set mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the image preprocessing: center-crop the input images so that their resolution is conformed to what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Compose([\n",
    "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(input_resolution),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, text need to be preprocessed. Let's use a case-insensitive tokenizer (to be preliminarily downloaded from [OpenAI](https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz)). Replace the placeholder in the *__init__* function of the class *SimpleTokenizer* below with the download location of the zipped pre-trained model on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = \"<put the full path of the zipped tokenizer here>\"):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve some local images. Set the *path* variable with a local directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '<put the full path of the directory containing the images here>'\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "all_images =[]\n",
    "for f in files:\n",
    "    image = PIL.Image.open(path + '\\\\' + f).convert(\"RGB\")\n",
    "    all_images.append(image)\n",
    "\n",
    "print(\"Total images retrived: \", len(all_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the retrived images within the notebook using [ipyplot](https://github.com/karolzak/ipyplot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 105\n",
    "img_width = 130\n",
    "ipyplot.plot_images(all_images, max_images=max_images, img_width=img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the retrived images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [preprocess(single_image) for single_image in all_images]\n",
    "\n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "image_input -= image_mean[:, None, None]\n",
    "image_input /= image_std[:, None, None]\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tokenizer and then define function two functions for semantic search, one to extract the text features from the search sentence and the second one to get top N images having semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "'''\n",
    "Given a sentence, this fuctions returns its text features. \n",
    "'''\n",
    "def get_text_features_from_search_phrase(phrase):\n",
    "    sentence_tokens = [tokenizer.encode(\"%s \"%(phrase) + \"<|endoftext|>\")]\n",
    "    text_input = torch.zeros(len(sentence_tokens), model.context_length, dtype=torch.long)\n",
    "    for idx, tokens in enumerate(sentence_tokens):\n",
    "        text_input[idx, :len(tokens)] = torch.tensor(tokens)\n",
    "    \n",
    "    text_input = text_input.cuda()\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_text(text_input).float()\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "'''\n",
    "Given a list of images and their similarity list, this function returns \n",
    "the similarity scores for the top images (along with the images themselves).\n",
    "'''\n",
    "def get_top_semantic_similarity(similarity_list, image_list, max_number_of_results):\n",
    "    results = zip(range(len(similarity_list)), similarity_list)\n",
    "    results = sorted(results, key=lambda x: x[1],reverse= True)\n",
    "    top_images = []\n",
    "    scores=[]\n",
    "    for index,score in results[:max_number_of_results]:\n",
    "        scores.append(score)\n",
    "        top_images.append(image_list[index])\n",
    "    return scores, top_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a search phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_phrase = \"set a search sentence here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the text features from the search phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = get_text_features_from_search_phrase(search_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top images having semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = similarity[0]\n",
    "max_number_of_results = 3\n",
    "scores, imgs = get_top_semantic_similarity(similarity, all_images, max_number_of_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the scores and plot the top images having semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity scores:\", scores)\n",
    "ipyplot.plot_images(imgs, img_width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
